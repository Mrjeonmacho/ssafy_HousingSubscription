import os
import chromadb
from chromadb.utils import embedding_functions
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# 1. í™˜ê²½ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(current_dir, "RAG_processed")  # C:\Users\SSAFY\Desktop\code\rag\vector\RAG_processed
db_path = os.path.join(current_dir, "chroma_db")    # C:\Users\SSAFY\Desktop\code\rag\vector\chroma_db
# í•œêµ­ì–´ ì „ìš© ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
ko_embedding = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="jhgan/ko-sroberta-multitask"
)

# ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
client = chromadb.PersistentClient(path=db_path)
collection = client.get_or_create_collection(
    name="happy_house_rag",
    embedding_function=ko_embedding
)

# ì²­í¬ ë¶„í• ê¸° ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name="gpt-4o",
    chunk_size=1500,
    chunk_overlap=200,
    separators=["\n\n", "\n", " "]
)

print("ğŸš€ ë°ì´í„° ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 2. í´ë” ìˆœíšŒ (ê° í´ë”ê°€ í•˜ë‚˜ì˜ 'ê³µê³ ' ë‹¨ìœ„)
for folder_name in os.listdir(base_dir):
    folder_path = os.path.join(base_dir, folder_name)

    # í´ë”ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
    if os.path.isdir(folder_path):
        combined_text = ""
        print(f"ğŸ“ í´ë” ì²˜ë¦¬ ì¤‘: {folder_name}")

        # 3. í´ë” ë‚´ íŒŒì¼ ìˆœíšŒ (TXTë§Œ ì¶”ì¶œ ë° ë³‘í•©)
        for file_name in os.listdir(folder_path):
            if file_name.endswith(".txt"):
                file_path = os.path.join(folder_path, file_name)
                with open(file_path, "r", encoding="utf-8") as f:
                    combined_text += f.read() + "\n\n"

        if not combined_text.strip():
            continue

        # 4. ì²­í¬ ë¶„í•  ì „ 'ì¶œì²˜ íƒœê·¸' ì£¼ì…
        # í…ìŠ¤íŠ¸ ë§¨ ì•ì— [ì¶œì²˜: í´ë”ëª…]ì„ ë¶™ì—¬ì„œ ë¶„í•  í›„ì—ë„ ëª¨ë“  ì¡°ê°ì´ ì¶œì²˜ë¥¼ ê¸°ì–µí•˜ê²Œ í•¨
        tagged_text = f"[ì¶œì²˜: {folder_name}]\n\n" + combined_text
        chunks = text_splitter.split_text(tagged_text)

        ids = []
        documents = []
        metadatas = []

        for i, chunk_content in enumerate(chunks):
            chunk_id = f"{folder_name}_chunk_{i+1}"

            ids.append(chunk_id)
            documents.append(chunk_content)

            # ë©”íƒ€ë°ì´í„°ì— í´ë”ëª…(source)ì„ ëª…ì‹œí•˜ì—¬ ë‚˜ì¤‘ì— í•„í„°ë§ ê°€ëŠ¥í•˜ê²Œ í•¨
            metadatas.append({
                "source": folder_name,
                "page": int(i * 1.5) + 1
            })

        # 5. DB ì €ì¥ (í´ë” ë‹¨ìœ„ë¡œ ì €ì¥)
        collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
        print(f"âœ… {folder_name} ì €ì¥ ì™„ë£Œ! (ì²­í¬: {len(chunks)}ê°œ)")

print("\nâœ¨ ëª¨ë“  ê³µê³  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ChromaDBì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì „ì²´ ê°œìˆ˜ í™•ì¸
print(f"ì „ì²´ ë°ì´í„° ê°œìˆ˜: {collection.count()}")

# ì €ì¥ëœ ì†ŒìŠ¤ ëª©ë¡ í™•ì¸
all_data = collection.get(include=['metadatas'])
sources = set(m['source'] for m in all_data['metadatas'])
print(f"ì €ì¥ëœ ê³µê³  ëª©ë¡: {sources}")